{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja871rAui_Da"
   },
   "source": [
    "# Scraping Files from Websites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwHrS-KJi_Dg"
   },
   "source": [
    "### You need to create a data set that tracks how many companies the <a href=\"https://www.sec.gov/litigation/suspensions.shtml\">SEC suspended</a> between 2019 and 1999. You find the data at:\n",
    "\n",
    "```https://www.sec.gov/litigation/suspensions.shtml```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_OAOaqLi_Dh"
   },
   "source": [
    "### We want to write a scraper that aggregates:\n",
    "\n",
    "* Date of suspension\n",
    "* Company name\n",
    "* Order\n",
    "* Release (the PDFs in the XX-YYYYY format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECK4Q7IOi_Di"
   },
   "source": [
    "# The Challenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQj34GvYi_Di"
   },
   "source": [
    "### Details are actually in PDFs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfkXDlQvi_Di"
   },
   "source": [
    "# Demo downloading files from websites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyvf5HLGi_Dj"
   },
   "source": [
    "There are ```txt``` and ```pdf``` files on:\n",
    "\n",
    "<a href=\"https://sandeepmj.github.io/scrape-example-page/pages.html\">Download documents</a>\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Download all ```txt``` files.\n",
    "2. Download all ```pdf``` files.\n",
    "3. Download all files as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: icecream in /Users/angeladegbesan/opt/anaconda3/lib/python3.8/site-packages (2.1.1)\n",
      "Requirement already satisfied: executing>=0.3.1 in /Users/angeladegbesan/opt/anaconda3/lib/python3.8/site-packages (from icecream) (0.8.1)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /Users/angeladegbesan/opt/anaconda3/lib/python3.8/site-packages (from icecream) (0.4.4)\n",
      "Requirement already satisfied: pygments>=2.2.0 in /Users/angeladegbesan/opt/anaconda3/lib/python3.8/site-packages (from icecream) (2.8.1)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in /Users/angeladegbesan/opt/anaconda3/lib/python3.8/site-packages (from icecream) (2.0.5)\n",
      "Requirement already satisfied: six in /Users/angeladegbesan/opt/anaconda3/lib/python3.8/site-packages (from asttokens>=2.0.1->icecream) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W1jQ2nQei_Dj"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup  ## scrape info from web pages\n",
    "import requests ## get web pages from server\n",
    "import time # time is required. we will use its sleep function\n",
    "from random import randrange # generate random numbers\n",
    "from icecream import ic\n",
    "# from google.colab import files ## code for downloading in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cV7Mwt-Ai_Dk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| response.status_code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url to scrape\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/pages.html\"\n",
    "response = requests.get(url)\n",
    "ic(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyHHqpmYi_Dk"
   },
   "source": [
    "## Turn page into soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dIkCJBH8i_Dk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <!-- Makes the page responsive and scaled to be read easily -->\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <!-- Links to stylesheet -->\n",
      "  <link href=\"style.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <!-- Remember to update page title -->\n",
      "  <title>\n",
      "   List of Documents\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <!-- All content goes here -->\n",
      "  <div class=\"container\">\n",
      "   <h1>\n",
      "    Documents to Download\n",
      "   </h1>\n",
      "   <li>\n",
      "    Junk Li\n",
      "    <a href=\"\">\n",
      "     tag 1\n",
      "    </a>\n",
      "   </li>\n",
      "   <li>\n",
      "    Junk Li\n",
      "    <a href=\"\">\n",
      "     tag 2\n",
      "    </a>\n",
      "   </li>\n",
      "   <ul class=\"txts downloadable\">\n",
      "    <p class=\"pages\">\n",
      "     Download this list of text documents\n",
      "    </p>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_01.txt\">\n",
      "      1\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_02.txt\">\n",
      "      2\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_03.txt\">\n",
      "      3\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_04.txt\">\n",
      "      4\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_05.txt\">\n",
      "      5\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_06.txt\">\n",
      "      6\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_07.txt\">\n",
      "      7\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_08.txt\">\n",
      "      8\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_09.txt\">\n",
      "      9\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     Text Document\n",
      "     <a href=\"files/text_doc_10.txt\">\n",
      "      10\n",
      "     </a>\n",
      "    </li>\n",
      "   </ul>\n",
      "   <li>\n",
      "    Junk Li\n",
      "    <a href=\"\">\n",
      "     tag 3\n",
      "    </a>\n",
      "   </li>\n",
      "   <li>\n",
      "    Junk Li\n",
      "    <a href=\"\">\n",
      "     tag 4\n",
      "    </a>\n",
      "   </li>\n",
      "   <ul class=\"pdfs downloadable\">\n",
      "    <p class=\"pages\">\n",
      "     Download this list of PDFs\n",
      "    </p>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_1.pdf\">\n",
      "      1\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_2.pdf\">\n",
      "      2\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_3.pdf\">\n",
      "      3\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_4.pdf\">\n",
      "      4\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_5.pdf\">\n",
      "      5\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_6.pdf\">\n",
      "      6\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_7.pdf\">\n",
      "      7\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_8.pdf\">\n",
      "      8\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_9.pdf\">\n",
      "      9\n",
      "     </a>\n",
      "    </li>\n",
      "    <li>\n",
      "     PDF Document\n",
      "     <a href=\"files/pdf_10.pdf\">\n",
      "      10\n",
      "     </a>\n",
      "    </li>\n",
      "   </ul>\n",
      "   <li>\n",
      "    Junk Li\n",
      "    <a href=\"\">\n",
      "     tag 5\n",
      "    </a>\n",
      "   </li>\n",
      "   <li>\n",
      "    Junk Li\n",
      "    <a href=\"\">\n",
      "     tag 6\n",
      "    </a>\n",
      "   </li>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "## get url and print but hard to read. will do prettify next\n",
    "soup = BeautifulSoup (response.text, \"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z42CN1eKi_Dk"
   },
   "source": [
    "## Find all txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l5GhvRuji_Dl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| txt_holder: [<ul class=\"txts downloadable\">\n",
      "                <p class=\"pages\">Download this list of text documents</p>\n",
      "                <li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
      "                <li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
      "                <li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
      "                <li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
      "                <li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
      "                <li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
      "                <li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
      "                <li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
      "                <li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
      "                <li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
      "                </ul>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<ul class=\"txts downloadable\">\n",
       " <p class=\"pages\">Download this list of text documents</p>\n",
       " <li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
       " <li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save in list called txt_holder\n",
    "txt_holder = soup.find_all(\"ul\", class_=\"txts\")\n",
    "ic(txt_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8sCo7syi_Dl"
   },
   "source": [
    "## Find all the ```a``` tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1BBjQWqsi_Dl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| txt_files_links: [<a href=\"files/text_doc_01.txt\">1</a>,\n",
      "                      <a href=\"files/text_doc_02.txt\">2</a>,\n",
      "                      <a href=\"files/text_doc_03.txt\">3</a>,\n",
      "                      <a href=\"files/text_doc_04.txt\">4</a>,\n",
      "                      <a href=\"files/text_doc_05.txt\">5</a>,\n",
      "                      <a href=\"files/text_doc_06.txt\">6</a>,\n",
      "                      <a href=\"files/text_doc_07.txt\">7</a>,\n",
      "                      <a href=\"files/text_doc_08.txt\">8</a>,\n",
      "                      <a href=\"files/text_doc_09.txt\">9</a>,\n",
      "                      <a href=\"files/text_doc_10.txt\">10</a>]\n"
     ]
    }
   ],
   "source": [
    "## for loop\n",
    "for txt_files in txt_holder:\n",
    "    txt_files_links = txt_files.find_all(\"a\")\n",
    "    ic (txt_files_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_files_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_files_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UPgFpi0ui_Dl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/text_doc_01.txt',\n",
       " 'files/text_doc_02.txt',\n",
       " 'files/text_doc_03.txt',\n",
       " 'files/text_doc_04.txt',\n",
       " 'files/text_doc_05.txt',\n",
       " 'files/text_doc_06.txt',\n",
       " 'files/text_doc_07.txt',\n",
       " 'files/text_doc_08.txt',\n",
       " 'files/text_doc_09.txt',\n",
       " 'files/text_doc_10.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## look at the links\n",
    "links = [txt_link.get(\"href\") for txt_link in txt_files_links]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3Sv-zUhi_Dm"
   },
   "source": [
    "## What is missing from the URLs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BR4K5BkGi_Dm"
   },
   "outputs": [],
   "source": [
    "## base url\n",
    "base_url = \"https://sandeepmj.github.io/scrape-example-page/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/text_doc_01.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_02.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_03.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_04.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_05.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_06.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_07.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_08.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_09.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_10.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [base_url + txt_link.get(\"href\") for txt_link in txt_files_links]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8sgMa05i_Dm"
   },
   "source": [
    "## Create a list of the full URLs\n",
    "\n",
    "Without all the ```html```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOJEvd2ki_Dm"
   },
   "outputs": [],
   "source": [
    "## lc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCqmE0y6i_Dm"
   },
   "source": [
    "## Download all the ```txt``` documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=5cd7d9c3f6939eb7ae6b21dcc53b4d521560c8d73f5305a7341eff7352159a3f\n",
      "  Stored in directory: /Users/angeladegbesan/Library/Caches/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zUM7ToNEi_Dn"
   },
   "outputs": [],
   "source": [
    "import wget # can put down documents, files from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "X_aUuV5ci_Dn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_01.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1 of 10\n",
      "Snoozing for 7 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_02.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2 of 10\n",
      "Snoozing for 6 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_03.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3 of 10\n",
      "Snoozing for 7 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_04.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 4 of 10\n",
      "Snoozing for 7 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_05.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 5 of 10\n",
      "Snoozing for 5 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_06.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 6 of 10\n",
      "Snoozing for 9 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_07.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 7 of 10\n",
      "Snoozing for 4 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_08.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 8 of 10\n",
      "Snoozing for 4 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_09.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 9 of 10\n",
      "Snoozing for 7 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| link: 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_10.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 10 of 10\n",
      "Snoozing for 8 seconds before next link\n"
     ]
    }
   ],
   "source": [
    "## download with timer\n",
    "link_number = len(links)\n",
    "link_count = 1\n",
    "\n",
    "#go through each link, to flag what it is downloading\n",
    "for link in links:           ##for link in links[:3]: if you want just the first 3 to test it out\n",
    "    ic(link)\n",
    "    print (f\"Downloaded {link_count} of {link_number}\")\n",
    "    link_count += 1 \n",
    "##downloading\n",
    "    wget.download(link)\n",
    "#snoozing and telling me for how long before the next downloads\n",
    "    mysnoozerbug = randrange (4,10)     \n",
    "    print (f\"Snoozing for {mysnoozerbug} seconds before next link\") \n",
    "    time.sleep(mysnoozerbug)    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zt6eKURbi_Dn"
   },
   "source": [
    "# Find all ```pdf``` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "T0aipPxyi_Dn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| pdf_holder: [<ul class=\"pdfs downloadable\">\n",
      "                <p class=\"pages\">Download this list of PDFs</p>\n",
      "                <li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
      "                <li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
      "                <li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
      "                <li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
      "                <li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
      "                <li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
      "                <li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
      "                <li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
      "                <li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
      "                <li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
      "                </ul>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<ul class=\"pdfs downloadable\">\n",
       " <p class=\"pages\">Download this list of PDFs</p>\n",
       " <li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
       " <li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## grab pdfs\n",
    "pdf_holder = soup.find_all(\"ul\", class_=\"pdfs\")\n",
    "ic(pdf_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| pdfake_holder: [<a href=\"files/pdf_1.pdf\">1</a>,\n",
      "                    <a href=\"files/pdf_2.pdf\">2</a>,\n",
      "                    <a href=\"files/pdf_3.pdf\">3</a>,\n",
      "                    <a href=\"files/pdf_4.pdf\">4</a>,\n",
      "                    <a href=\"files/pdf_5.pdf\">5</a>,\n",
      "                    <a href=\"files/pdf_6.pdf\">6</a>,\n",
      "                    <a href=\"files/pdf_7.pdf\">7</a>,\n",
      "                    <a href=\"files/pdf_8.pdf\">8</a>,\n",
      "                    <a href=\"files/pdf_9.pdf\">9</a>,\n",
      "                    <a href=\"files/pdf_10.pdf\">10</a>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/pdf_1.pdf\">1</a>,\n",
       " <a href=\"files/pdf_2.pdf\">2</a>,\n",
       " <a href=\"files/pdf_3.pdf\">3</a>,\n",
       " <a href=\"files/pdf_4.pdf\">4</a>,\n",
       " <a href=\"files/pdf_5.pdf\">5</a>,\n",
       " <a href=\"files/pdf_6.pdf\">6</a>,\n",
       " <a href=\"files/pdf_7.pdf\">7</a>,\n",
       " <a href=\"files/pdf_8.pdf\">8</a>,\n",
       " <a href=\"files/pdf_9.pdf\">9</a>,\n",
       " <a href=\"files/pdf_10.pdf\">10</a>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfake_holder = soup.find(\"ul\", class_=\"pdfs\").find_all(\"a\")     ##faster way since we only have one \"ul\" with class \"pdfs\"\n",
    "ic(pdfake_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUEScUwei_Dn"
   },
   "source": [
    "## Find all the ```a``` tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "DohlMXf2i_Do",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| pdf_files_links: [<a href=\"files/pdf_1.pdf\">1</a>,\n",
      "                      <a href=\"files/pdf_2.pdf\">2</a>,\n",
      "                      <a href=\"files/pdf_3.pdf\">3</a>,\n",
      "                      <a href=\"files/pdf_4.pdf\">4</a>,\n",
      "                      <a href=\"files/pdf_5.pdf\">5</a>,\n",
      "                      <a href=\"files/pdf_6.pdf\">6</a>,\n",
      "                      <a href=\"files/pdf_7.pdf\">7</a>,\n",
      "                      <a href=\"files/pdf_8.pdf\">8</a>,\n",
      "                      <a href=\"files/pdf_9.pdf\">9</a>,\n",
      "                      <a href=\"files/pdf_10.pdf\">10</a>]\n"
     ]
    }
   ],
   "source": [
    "## for loop store in all_pdf_links_fl\n",
    "for pdf_files in pdf_holder:\n",
    "    pdf_files_links = pdf_files.find_all(\"a\")\n",
    "    ic (pdf_files_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_link = [base_url + pdf_file.get(\"href\") for pdf_file in pdf_files_links]\n",
    "p_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfSN4n4Ki_Do"
   },
   "source": [
    "## Find all the ```a``` tags \n",
    "\n",
    "Without all the ```html```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hp00Mel0i_Do"
   },
   "outputs": [],
   "source": [
    "## lc store in all_pdf_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8QfQI1Qi_Do"
   },
   "source": [
    "## Download all the ```pdf``` documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "W7-meNFri_Do"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1 of 10\n",
      "Snoozing for 5 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2 of 10\n",
      "Snoozing for 3 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3 of 10\n",
      "Snoozing for 3 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 4 of 10\n",
      "Snoozing for 3 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 5 of 10\n",
      "Snoozing for 2 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 6 of 10\n",
      "Snoozing for 4 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 7 of 10\n",
      "Snoozing for 4 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 8 of 10\n",
      "Snoozing for 3 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 9 of 10\n",
      "Snoozing for 2 seconds before next link\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| plink: 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 10 of 10\n",
      "Snoozing for 5 seconds before next link\n"
     ]
    }
   ],
   "source": [
    "plink_number = len(links)\n",
    "plink_count = 1\n",
    "\n",
    "#go through each link, to flag what it is downloading\n",
    "for plink in p_link:           ##for link in links[:3]: if you want just the first 3 to test it out\n",
    "    ic(plink)\n",
    "    print (f\"Downloaded {plink_count} of {plink_number}\")\n",
    "    plink_count += 1 \n",
    "##downloading\n",
    "    wget.download(plink)\n",
    "#snoozing and telling me for how long before the next downloads\n",
    "    mysnoozerpbug = randrange (2,6)     \n",
    "    print (f\"Snoozing for {mysnoozerpbug} seconds before next link\") \n",
    "    time.sleep(mysnoozerpbug)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLOjF-XKi_Do"
   },
   "source": [
    "# Find all the files and download at one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "XmMZ8b0Di_Dp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul class=\"txts downloadable\">\n",
       " <p class=\"pages\">Download this list of text documents</p>\n",
       " <li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
       " <li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
       " </ul>,\n",
       " <ul class=\"pdfs downloadable\">\n",
       " <p class=\"pages\">Download this list of PDFs</p>\n",
       " <li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
       " <li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## find all files in our soup\n",
    "all_holder = soup.find_all (\"ul\", class_=\"downloadable\")\n",
    "all_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ul class=\"txts downloadable\">\n",
      "<p class=\"pages\">Download this list of text documents</p>\n",
      "<li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
      "<li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
      "<li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
      "<li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
      "<li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
      "<li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
      "<li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
      "<li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
      "<li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
      "<li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
      "</ul>\n",
      "**********\n",
      "<ul class=\"pdfs downloadable\">\n",
      "<p class=\"pages\">Download this list of PDFs</p>\n",
      "<li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
      "<li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
      "<li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
      "<li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
      "<li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
      "<li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
      "<li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
      "<li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
      "<li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
      "<li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
      "</ul>\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for item in all_holder:\n",
    "    print (item)\n",
    "    print (\"**********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIAFCQvci_Dp"
   },
   "source": [
    "## Stop...we can't throw such a wide net!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Toth3b6ui_Dp"
   },
   "source": [
    "# Target the class ```downloadable```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6mKi4RWi_Dp"
   },
   "outputs": [],
   "source": [
    "## find all files in our soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grnOHd9di_Dp"
   },
   "outputs": [],
   "source": [
    "## type?\n",
    "type(docs_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGQeldzsi_Dp"
   },
   "source": [
    "### We run into problems because we have a list of lists\n",
    "\n",
    "#### Quick detour to flatten list lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ghmEL1-1i_Dq",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>,\n",
       "  <li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>],\n",
       " [<li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>,\n",
       "  <li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## because docs_holder has p tags, newlines, etc. we need to focus it\n",
    "all_li = [item.find_all(\"li\") for item in all_holder]\n",
    "all_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       "  <a href=\"files/text_doc_02.txt\">2</a>,\n",
       "  <a href=\"files/text_doc_03.txt\">3</a>,\n",
       "  <a href=\"files/text_doc_04.txt\">4</a>,\n",
       "  <a href=\"files/text_doc_05.txt\">5</a>,\n",
       "  <a href=\"files/text_doc_06.txt\">6</a>,\n",
       "  <a href=\"files/text_doc_07.txt\">7</a>,\n",
       "  <a href=\"files/text_doc_08.txt\">8</a>,\n",
       "  <a href=\"files/text_doc_09.txt\">9</a>,\n",
       "  <a href=\"files/text_doc_10.txt\">10</a>],\n",
       " [<a href=\"files/pdf_1.pdf\">1</a>,\n",
       "  <a href=\"files/pdf_2.pdf\">2</a>,\n",
       "  <a href=\"files/pdf_3.pdf\">3</a>,\n",
       "  <a href=\"files/pdf_4.pdf\">4</a>,\n",
       "  <a href=\"files/pdf_5.pdf\">5</a>,\n",
       "  <a href=\"files/pdf_6.pdf\">6</a>,\n",
       "  <a href=\"files/pdf_7.pdf\">7</a>,\n",
       "  <a href=\"files/pdf_8.pdf\">8</a>,\n",
       "  <a href=\"files/pdf_9.pdf\">9</a>,\n",
       "  <a href=\"files/pdf_10.pdf\">10</a>]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_a = [item.find_all(\"a\") for item in all_holder]\n",
    "all_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkYtIzr_i_Dq"
   },
   "source": [
    "## itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "F729Ln8ci_Dq",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       " <a href=\"files/text_doc_02.txt\">2</a>,\n",
       " <a href=\"files/text_doc_03.txt\">3</a>,\n",
       " <a href=\"files/text_doc_04.txt\">4</a>,\n",
       " <a href=\"files/text_doc_05.txt\">5</a>,\n",
       " <a href=\"files/text_doc_06.txt\">6</a>,\n",
       " <a href=\"files/text_doc_07.txt\">7</a>,\n",
       " <a href=\"files/text_doc_08.txt\">8</a>,\n",
       " <a href=\"files/text_doc_09.txt\">9</a>,\n",
       " <a href=\"files/text_doc_10.txt\">10</a>,\n",
       " <a href=\"files/pdf_1.pdf\">1</a>,\n",
       " <a href=\"files/pdf_2.pdf\">2</a>,\n",
       " <a href=\"files/pdf_3.pdf\">3</a>,\n",
       " <a href=\"files/pdf_4.pdf\">4</a>,\n",
       " <a href=\"files/pdf_5.pdf\">5</a>,\n",
       " <a href=\"files/pdf_6.pdf\">6</a>,\n",
       " <a href=\"files/pdf_7.pdf\">7</a>,\n",
       " <a href=\"files/pdf_8.pdf\">8</a>,\n",
       " <a href=\"files/pdf_9.pdf\">9</a>,\n",
       " <a href=\"files/pdf_10.pdf\">10</a>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's use itertools to flatten the list\n",
    "flat_list = list(itertools.chain(*all_a))\n",
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/text_doc_01.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_02.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_03.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_04.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_05.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_06.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_07.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_08.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_09.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_10.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_links = [base_url + allfile.get(\"href\") for allfile in flat_list]\n",
    "n_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKg_eOsOi_Dq"
   },
   "outputs": [],
   "source": [
    "## let's blend BeautifulSoup and itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA22k16Wi_Dq"
   },
   "source": [
    "## For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUe6Ikfri_Dq"
   },
   "outputs": [],
   "source": [
    "## Flatten via for loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZH5X9IBi_Dr"
   },
   "source": [
    "## List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXBeF0ubi_Dr"
   },
   "outputs": [],
   "source": [
    "# step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oToaTbmKi_Dr"
   },
   "outputs": [],
   "source": [
    "# step 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wjrnhrui_Dr"
   },
   "source": [
    "## Download all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXpDuRyvi_Dr"
   },
   "outputs": [],
   "source": [
    "## careful to put in a list name we just processed (via lc, fl, itertools)\n",
    "nlink_number = len(links)\n",
    "nlink_count = 1\n",
    "\n",
    "#go through each link, to flag what it is downloading\n",
    "for nlink in n_links:           ##for link in links[:3]: if you want just the first 3 to test it out\n",
    "    ic(nlink)\n",
    "    print (f\"Downloaded {nlink_count} of {nlink_number}\")\n",
    "    nlink_count += 1 \n",
    "##downloading\n",
    "    wget.download(nlink)\n",
    "#snoozing and telling me for how long before the next downloads\n",
    "    mysnoozerbug = randrange (4,10)     \n",
    "    print (f\"Snoozing for {mysnoozerbug} seconds before next link\") \n",
    "    time.sleep(mysnoozerbug) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "week-7-download_docs_BLANK.ipynb",
   "provenance": [
    {
     "file_id": "1J1lhUQakLF4NmWxBeEGiprUMSv1j7cpk",
     "timestamp": 1628019785208
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
